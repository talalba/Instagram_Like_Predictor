{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eX5OmYhG4_NH6F-yUkMdZxne78YylTyP",
      "authorship_tag": "ABX9TyOUlOy9Ir2Ntdj8o9JXy7ah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talalba/Instagram_Like_Predictor/blob/master/lid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi7DnjgwhXRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab53e53c-ec61-4158-bbda-cf2e26fc596c"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import importlib\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from random import sample \n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "from string import digits \n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZdDe02S63Ic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "981d0af0-11f2-408b-be12-beb43e72e8ba"
      },
      "source": [
        "# load the data\n",
        "directoryPath = '/content/drive/My Drive/lid/'\n",
        "# df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';',encoding='latin1')\n",
        "df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';')\n",
        "df.shape\n",
        "\n",
        "# get the language data\n",
        "text = df['text'].tolist()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opxnPii3FNpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter the text\n",
        "def filter_data(text):\n",
        "\n",
        "  # define digits which will be removed from the string\n",
        "  remove_digits = str.maketrans('', '', digits) \n",
        "\n",
        "  docs = [x.translate(remove_digits) for x in text]   # remove numbers in strings\n",
        "  docs = [x.lower() for x in docs] # make letters lower case\n",
        "  docs = [x.strip() for x in docs] # remove white space \n",
        "  # docs = [x.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~')) for x in docs] # remove punctuation\n",
        "  # docs = [x.translate(str.maketrans('', '',   string.punctuation )) for x in docs] # remove punctuation\n",
        "\n",
        "  return docs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfW42DOEi0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa53d35e-8beb-4971-c2bb-e219507b62a9"
      },
      "source": [
        "filtered_text = filter_data(text)\n",
        "\n",
        "# get sentences from paragraphs\n",
        "# ? do periods mark sentences\n",
        "filtered_text_sen = [sent_tokenize(x) for x in filtered_text]\n",
        "\n",
        "len(filtered_text_sen[0])\n",
        "len(text[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5JOe9iRlFAV",
        "colab_type": "code",
        "outputId": "8bbd1090-d989-419e-997d-1f02ca3f7358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# get performance stats\n",
        "no_train_sentences = 200\n",
        "no_test_sentences = 1\n",
        "no_runs = 1 # number of times to run the classifier\n",
        "ngram_number = 1\n",
        "target_names = df['language'].tolist()\n",
        "predicted_labels = []\n",
        "actual_labels = list(range(0,df.shape[0]))*no_runs\n",
        "\n",
        "# do n numher of runs and get predicted languages \n",
        "for ii in tqdm(range(0,no_runs)):\n",
        "  predicted_labels = np.append(predicted_labels,get_performance_stats(no_train_sentences,no_test_sentences,filtered_text_sen,ngram_number))\n",
        "\n",
        "# prin classification performance \n",
        "cr = classification_report( actual_labels,predicted_labels, target_names=target_names)\n",
        "print(cr)\n",
        "\n",
        "# print confusion matrix\n",
        "cm = confusion_matrix(actual_labels, predicted_labels)\n",
        "print(cm)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          bg       0.50      1.00      0.67         1\n",
            "          cs       1.00      1.00      1.00         1\n",
            "          da       1.00      1.00      1.00         1\n",
            "          de       1.00      1.00      1.00         1\n",
            "          el       1.00      1.00      1.00         1\n",
            "          en       1.00      1.00      1.00         1\n",
            "          es       1.00      1.00      1.00         1\n",
            "          et       0.00      0.00      0.00         1\n",
            "          fi       0.50      1.00      0.67         1\n",
            "          fr       1.00      1.00      1.00         1\n",
            "          hu       1.00      1.00      1.00         1\n",
            "          it       1.00      1.00      1.00         1\n",
            "          lt       1.00      1.00      1.00         1\n",
            "          lv       0.00      0.00      0.00         1\n",
            "          nl       1.00      1.00      1.00         1\n",
            "          pl       1.00      1.00      1.00         1\n",
            "          pt       1.00      1.00      1.00         1\n",
            "          ro       1.00      1.00      1.00         1\n",
            "          sk       1.00      1.00      1.00         1\n",
            "          sl       1.00      1.00      1.00         1\n",
            "          sv       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.86        21\n",
            "   macro avg       0.81      0.86      0.83        21\n",
            "weighted avg       0.81      0.86      0.83        21\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kubk7jkRG38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_performance_stats(no_train_sentences,no_test_sentences,data,ngram_number):\n",
        "\n",
        "  # split_data = lambda x:  sample(x,no_train_sentences + no_test_sentences)\n",
        "\n",
        "  # z = list(map(split_data, data))\n",
        "\n",
        "  data = filtered_text_sen\n",
        "\n",
        "  # split data into train and test set\n",
        "  z = [sample(x,no_train_sentences + no_test_sentences) for x in data]\n",
        "\n",
        "  # index the train and test data from the list z\n",
        "  train_data = [item[:no_train_sentences] for item in z]\n",
        "  test_data = [item[-no_test_sentences:] for item in z]\n",
        "\n",
        "  def remove_punctuation(from_text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in from_text]\n",
        "    return stripped\n",
        "\n",
        "  train_data = [remove_punctuation(i) for i in train_data]\n",
        "  test_data = [remove_punctuation(i) for i in test_data]\n",
        "\n",
        "  # combine the list of sentences to a list of strings \n",
        "  train_data = [''.join(x) for x in train_data]\n",
        "  test_data = [''.join(x) for x in test_data]\n",
        "\n",
        "  # # remove punctuation from the string\n",
        "  # train_data2 = [remove_punctuation(text) for text in train_data]\n",
        "  # test_data2 = [remove_punctuation(text) for text in test_data]\n",
        "\n",
        "  # len(test_data2)\n",
        "\n",
        "  # pd.DataFrame([D.idxmin(axis=1), pd.Series(target_names)],index=['predicted','actual'])\n",
        "  # D.idxmin(axis=1).where(D.idxmin(axis=1)==target_names)\n",
        "\n",
        "  dist_mat = get_language_id(ngram_number,train_data,test_data)\n",
        "\n",
        "  return dist_mat.idxmin(axis=1).tolist()\n",
        "\n",
        "\n",
        "  # y_true = df['language'].index.values\n",
        "  # y_predicted = D.idxmin(axis=1)\n",
        "  # target_names = df['language'].tolist()\n",
        "  # # len(target_names)\n",
        "  # # pd.crosstab(df['language'],D.idxmin(axis=1))\n",
        "\n",
        "  # print(classification_report( df['language'], D.idxmin(axis=1), target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxi6WRuwF9lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function performs language identification given train and test data and the ngram number \n",
        "# train_data, test_data contain a list for each language\n",
        "\n",
        "def get_language_id(ngram_number,train_data2,test_data2):\n",
        "    \n",
        "  # perform statistics on training data for all languages\n",
        "  # define function to get unigram frequencies \n",
        "  get_frequency_unigram = lambda x:  nltk.FreqDist((ngrams(' '.join(map(str, x)), 2)))\n",
        "  # get_frequency_unigram = lambda x:  nltk.FreqDist((ngrams(x, ngram_number)))\n",
        "\n",
        "  # nltk.FreqDist((ngrams(train_data[0], ngram_number)))\n",
        "  # list((ngrams(' '.join(map(str, train_data[0])), ngram_number)))\n",
        "  # list(nltk.FreqDist((ngrams(' '.join(map(str, train_data[0])), ngram_number))))                         \n",
        "  # padded_ngrams = partial(ngrams, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')\n",
        "\n",
        "  # ee = [(ngrams(x, 1)) for x in train_data[5]]\n",
        "  # get ngram counts for train and test data\n",
        "  # list_ngram_train = [nltk.FreqDist((ngrams(x, ngram_number))) for x in ee]\n",
        "  list_ngram_train = list(map(get_frequency_unigram, train_data2))\n",
        "\n",
        "  list_ngram_test = list(map(get_frequency_unigram, test_data2))\n",
        "  \n",
        "  # list_unigram_train = list(map(get_frequency_unigram, train_data))\n",
        "  # list_unigram_test = list(map(get_frequency_unigram, test_data))\n",
        "  # make dataframe\n",
        "  train_data_df = pd.DataFrame(list_ngram_train)\n",
        "  test_data_df = pd.DataFrame(list_ngram_test)\n",
        "\n",
        "  # test_data_df = pd.DataFrame(list_ngram_test)\n",
        "\n",
        "  # a count of 0.5 is added to events seen in at least\n",
        "  # one language in the training set\n",
        "  np.max(train_data_df.isna().sum())\n",
        "\n",
        "  # all colums have atleast one nonempty values given how python constructs dictionaries\n",
        "  # thus we replace all NAs with .5\n",
        "  train_data_df = train_data_df.fillna(.5)\n",
        "\n",
        "  # get probabilities from counts by dividing the sum\n",
        "  train_data_df = train_data_df.div(train_data_df.sum(axis=1), axis=0)\n",
        "  test_data_df = test_data_df.div(test_data_df.sum(axis=1), axis=0)\n",
        "  \n",
        "  try:\n",
        "\n",
        "    # get common ngram columns between train and test data\n",
        "    common_cols =  np.intersect1d(train_data_df.columns, test_data_df.columns)\n",
        "    len(common_cols)\n",
        "\n",
        "    train_data_df2 = train_data_df[common_cols]\n",
        "    test_data_df2 = test_data_df[common_cols]\n",
        "\n",
        "    # D = pd.DataFrame(index=train_data_df.index, columns=train_data_df.index)\n",
        "\n",
        "    D = pd.DataFrame(columns=train_data_df.index)\n",
        "    \n",
        "    # perform language indetification using relative entropy\n",
        "    for ii in range(0,len(test_data)):\n",
        "\n",
        "      # compute the KL distance \n",
        "      # D = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "      # D.iloc[ii,:] = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "      d = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "\n",
        "      D = D.append(d, ignore_index=True)\n",
        "\n",
        "\n",
        "      # find index of the train language which has the minimum score (closest language in the train set)\n",
        "      # index = np.argmin(D.iloc[ii,:])\n",
        "      \n",
        "      # print(index)\n",
        "  except:\n",
        "    print(\"No mtaching ngrams found between test and train data\")\n",
        "\n",
        "\n",
        "  return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgMT6cwtZWy9",
        "colab_type": "code",
        "outputId": "c78db698-78a1-461e-b270-e7815165a539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# to test one sentence or more\n",
        "test_sentence = ['I am currently eating my breakfast','I will go to school']\n",
        "test_sentence = pd.DataFrame(['J’ai oublié mon parapluie dans l’abribus'],columns = ['text'])\n",
        "\n",
        "\n",
        "filtered_text = filter_data(test_sentence)\n",
        "\n",
        "# get sentences from paragraphs\n",
        "# ? do periods mark sentences\n",
        "filtered_test_sen = [sent_tokenize(x) for x in filtered_text]\n",
        "\n",
        "# remove punctuation from the string\n",
        "filtered_test_sen2 = [remove_punctuation(text) for text in filtered_test_sen]\n",
        "\n",
        "# for unigrams don't include the white space as a feature\n",
        "if (ngram_number==1):\n",
        "  test_data2 = [text.replace(\" \", \"\") for text in test_data2] \n",
        "\n",
        "len(test_data2)\n",
        "distance_mat = get_language_id(1,train_data,filtered_test_sen)\n",
        "\n",
        "\n",
        "# test_data2 = test_data\n",
        "# train_data2 = train_data2\n",
        "\n",
        "distance_mat.idxmin(axis=1).to_string(index=False)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-a3dc7ef879fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistance_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_language_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiltered_test_sen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-80e5dcf39376>\u001b[0m in \u001b[0;36mget_language_id\u001b[0;34m(ngram_number, train_data2, test_data2)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# D = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# D.iloc[ii,:] = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_df2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2068\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iObEQwV9R08N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Highlighting the limits of your model\n",
        "\n",
        "# no spatial order\n",
        "# size of the feature space increase as bigger n grams are used\n",
        "# charcter level doesnot capture word level information, combin word and character levle features?\n",
        "# doesnot work well for bery short sentences as the probailities are not accurate\n",
        "# use PCA dimentionality reduction\n",
        "# skipe gram model, graph based tp capture surrounding word distributions "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalAAc9PQW7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # prepare the train and test data\n",
        "\n",
        "# # split data into train and test set\n",
        "# test_data_size = .8 # use 20% data for testing, 80% for training\n",
        "\n",
        "# # get all the words for each language and save it in a list\n",
        "# word_list = [i.split() for i in  docs]\n",
        "\n",
        "# # generate training and test data for each language using a map fucntion\n",
        "# # we will split the words in each language into train and test\n",
        "# # this ignores the temporal order of the words\n",
        "# split_data = lambda x:  train_test_split(x,test_size=test_data_size)\n",
        "\n",
        "# # get a list which contains the test and train data splits\n",
        "# z = list(map(split_data, word_list))\n",
        "# # z2 = [train_test_split(x,test_size=.2,radnom_state=42) for x in word_list];\n",
        "\n",
        "# # index the train and test data from the list z\n",
        "# train_data = [item[0] for item in z]\n",
        "# test_data = [item[1] for item in z]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}