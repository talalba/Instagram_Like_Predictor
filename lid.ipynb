{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eX5OmYhG4_NH6F-yUkMdZxne78YylTyP",
      "authorship_tag": "ABX9TyMS+ZNlU2YSZbntXMqrjmL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talalba/Instagram_Like_Predictor/blob/master/lid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi7DnjgwhXRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import importlib\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an8lgCXzgV9F",
        "colab_type": "code",
        "outputId": "c2508561-9625-4f55-f011-5d1c619ae7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the data\n",
        "directoryPath = '/content/drive/My Drive/lid/'\n",
        "# df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';',encoding='latin1')\n",
        "df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';')\n",
        "df.shape\n",
        "\n",
        "# function to remove punctuation \n",
        "def remove_punctuation(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "\n",
        "# function to remove whitespace \n",
        "def remove_whitespace(text): \n",
        "    return  \" \".join(text.split()) \n",
        "\n",
        "docs = df['text'].str.lower().values # make the words lower string\n",
        "docs = [remove_punctuation(text) for text in docs]\n",
        "docs = [remove_whitespace(text) for text in docs]\n",
        "\n",
        "# split data into train and test set\n",
        "test_data_size = .2 # use 20% data for testing, 80% for training\n",
        "\n",
        "# get all the words for each language and save it in a list\n",
        "word_list = [i.split() for i in  docs]\n",
        "\n",
        "# generate training and test data for each language using a map fucntion\n",
        "# we will split the words in each language into train and test\n",
        "split_data = lambda x:  train_test_split(x,test_size=test_data_size)\n",
        "\n",
        "\n",
        "# get a list which contains the test and train data splits\n",
        "z = list(map(split_data, word_list))\n",
        "# z2 = [train_test_split(x,test_size=.2,radnom_state=42) for x in word_list];\n",
        "\n",
        "# index the train and test data from the list z\n",
        "train_data = [item[0] for item in z]\n",
        "test_data = [item[1] for item in z]"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp0dqRIRNaKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "242c3b53-708c-454a-f163-6b10e4eca29d"
      },
      "source": [
        "get_language_id(1,train_data,test_data)"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxi6WRuwF9lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function performs language identification given train and test data and the ngram number \n",
        "# train_data, test_data contain a list for each language\n",
        "\n",
        "def get_language_id(ngram_number,train_data,test_data):\n",
        "    \n",
        "  # perform statistics on training data for all languages\n",
        "  # define function to get unigram frequencies \n",
        "  get_frequency_unigram = lambda x:  nltk.FreqDist((ngrams(' '.join(map(str, x)), ngram_number)))\n",
        "\n",
        "  # get unigram counts for train and test data\n",
        "  list_unigram_train = list(map(get_frequency_unigram, train_data))\n",
        "  list_unigram_test = list(map(get_frequency_unigram, test_data))\n",
        "\n",
        "  # make dataframe\n",
        "  train_data_df = pd.DataFrame(list_unigram_train,index=df['language'])\n",
        "  test_data_df = pd.DataFrame(list_unigram_test,index=df['language'])\n",
        "\n",
        "  # a count of 0.5 is added to events seen in at least\n",
        "  # one language in the training set\n",
        "  np.max(train_data_df.isna().sum())\n",
        "\n",
        "  # all colums have atleast one nonempty values given how python constructs dictionaries\n",
        "  # thus we replace all NAs with .5\n",
        "  train_data_df = train_data_df.fillna(.5)\n",
        "\n",
        "  # get probabilities from counts by dividing the sum\n",
        "  train_data_df = train_data_df.div(train_data_df.sum(axis=1), axis=0)\n",
        "  test_data_df = test_data_df.div(test_data_df.sum(axis=1), axis=0)\n",
        "\n",
        "  # get common ngram columns between train and test data\n",
        "  common_cols =  np.intersect1d(train_data_df.columns, test_data_df.columns)\n",
        "  len(common_cols)\n",
        "\n",
        "  train_data_df2 = train_data_df[common_cols]\n",
        "  test_data_df2 = test_data_df[common_cols]\n",
        "\n",
        "  # perform language indetification using relative entropy\n",
        "  for ii in range(0,len(list_unigram_test)):\n",
        "\n",
        "    # compute the KL distance \n",
        "    D = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "\n",
        "    # find index of the train language which has the minimum score (closest language in the train set)\n",
        "    index = np.argmin(D)\n",
        "\n",
        "    print(index)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}