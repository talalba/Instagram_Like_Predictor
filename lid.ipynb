{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eX5OmYhG4_NH6F-yUkMdZxne78YylTyP",
      "authorship_tag": "ABX9TyNgOptXZsV1htNk4iqESHbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talalba/Instagram_Like_Predictor/blob/master/lid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi7DnjgwhXRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ff8f89b4-8a48-447f-cadc-0933271df08c"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import importlib\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from random import sample \n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an8lgCXzgV9F",
        "colab_type": "code",
        "outputId": "b68d5ae7-f0a1-4299-8e42-70b09f634738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the data\n",
        "directoryPath = '/content/drive/My Drive/lid/'\n",
        "# df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';',encoding='latin1')\n",
        "df = pd.read_csv(directoryPath+'formatted_data.csv',sep=';')\n",
        "df.shape\n",
        "\n",
        "# filter the data to remove unwanted characters\n",
        "# function to remove punctuation \n",
        "def remove_punctuation(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "\n",
        "# do we keep whitespace?\n",
        "# function to remove whitespace \n",
        "def remove_whitespace(text): \n",
        "    return  \" \".join(text.split()) \n",
        "\n",
        "docs = df['text'].str.replace('\\d+', '') # remove numbers in strings\n",
        "docs = docs.str.lower() # make letters lower case\n",
        "docs = [remove_whitespace(text) for text in docs]\n",
        "\n",
        "# get sentences from paragraphs\n",
        "docs = [sent_tokenize(text) for text in docs]\n"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kubk7jkRG38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c4265925-4568-476e-d210-8e068c0a8d06"
      },
      "source": [
        "# split data into train and test set\n",
        "# number of training sentences\n",
        "no_train_sentences = 200\n",
        "no_test_sentences = 1\n",
        "split_data = lambda x:  sample(x,no_train_sentences + no_test_sentences)\n",
        "\n",
        "z = list(map(split_data, docs))\n",
        "\n",
        "# index the train and test data from the list z\n",
        "train_data = [item[:no_train_sentences] for item in z]\n",
        "test_data = [item[-no_test_sentences:] for item in z]\n",
        "\n",
        "# combine the list of senetnces to a list of strings \n",
        "train_data = [''.join(x) for x in train_data]\n",
        "test_data = [''.join(x) for x in test_data]\n",
        "\n",
        "# remove punctuation from the string\n",
        "train_data2 = [remove_punctuation(text) for text in train_data]\n",
        "test_data2 = [remove_punctuation(text) for text in test_data]\n",
        "\n",
        "D = get_language_id(1,train_data2,test_data2)\n",
        "D.idxmin(axis=1).to_string(index=False)\n"
      ],
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     bg\n",
              "1     sk\n",
              "2     da\n",
              "3     de\n",
              "4     el\n",
              "5     en\n",
              "6     es\n",
              "7     et\n",
              "8     fi\n",
              "9     fr\n",
              "10    hu\n",
              "11    it\n",
              "12    lt\n",
              "13    en\n",
              "14    en\n",
              "15    pl\n",
              "16    pt\n",
              "17    ro\n",
              "18    sk\n",
              "19    sl\n",
              "20    sv\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 481
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgMT6cwtZWy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to test one sentence or more\n",
        "test_sentence = pd.DataFrame(['I am going to play the game and conquer the world'],columns = ['text'])\n",
        "\n",
        "docs = test_sentence['text'].str.replace('\\d+', '') # remove numbers in strings\n",
        "docs = docs.str.lower() # make letters lower case\n",
        "test_data = [remove_whitespace(text) for text in docs]\n",
        "test_data = [''.join(x) for x in test_data]\n",
        "\n",
        "\n",
        "len(test_data2)\n",
        "# remove punctuation from the string\n",
        "test_data2 = [remove_punctuation(text) for text in test_data]\n",
        "distance_mat = get_language_id(1,train_data2,test_data2)\n",
        "\n",
        "\n",
        "test_data2 = test_data\n",
        "train_data2 = train_data2\n",
        "\n",
        "D.idxmin(axis=1).to_string(index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxi6WRuwF9lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function performs language identification given train and test data and the ngram number \n",
        "# train_data, test_data contain a list for each language\n",
        "\n",
        "def get_language_id(ngram_number,train_data,test_data):\n",
        "    \n",
        "  # perform statistics on training data for all languages\n",
        "  # define function to get unigram frequencies \n",
        "  get_frequency_unigram = lambda x:  nltk.FreqDist((ngrams(' '.join(map(str, x)), ngram_number)))\n",
        "\n",
        "  # get unigram counts for train and test data\n",
        "  list_unigram_train = list(map(get_frequency_unigram, train_data))\n",
        "  list_unigram_test = list(map(get_frequency_unigram, test_data))\n",
        "\n",
        "  # make dataframe\n",
        "  train_data_df = pd.DataFrame(list_unigram_train,index=df['language'])\n",
        "  test_data_df = pd.DataFrame(list_unigram_test,index=df['language'])\n",
        "\n",
        "  # a count of 0.5 is added to events seen in at least\n",
        "  # one language in the training set\n",
        "  np.max(train_data_df.isna().sum())\n",
        "\n",
        "  # all colums have atleast one nonempty values given how python constructs dictionaries\n",
        "  # thus we replace all NAs with .5\n",
        "  train_data_df = train_data_df.fillna(.5)\n",
        "\n",
        "  # get probabilities from counts by dividing the sum\n",
        "  train_data_df = train_data_df.div(train_data_df.sum(axis=1), axis=0)\n",
        "  test_data_df = test_data_df.div(test_data_df.sum(axis=1), axis=0)\n",
        "\n",
        "  # get common ngram columns between train and test data\n",
        "  common_cols =  np.intersect1d(train_data_df.columns, test_data_df.columns)\n",
        "  len(common_cols)\n",
        "\n",
        "  train_data_df2 = train_data_df[common_cols]\n",
        "  test_data_df2 = test_data_df[common_cols]\n",
        "\n",
        "  # D = pd.DataFrame(index=train_data_df.index, columns=train_data_df.index)\n",
        "\n",
        "  D = pd.DataFrame(columns=train_data_df.index)\n",
        "  \n",
        "\n",
        "\n",
        "  # perform language indetification using relative entropy\n",
        "  for ii in range(0,len(test_data)):\n",
        "\n",
        "    # compute the KL distance \n",
        "    # D = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "    # D.iloc[ii,:] = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "    d = (test_data_df2.iloc[ii,:]*np.log(test_data_df2.iloc[ii,:].div(train_data_df2))).sum(axis=1)\n",
        "\n",
        "    D = D.append(d, ignore_index=True)\n",
        "\n",
        "\n",
        "    # find index of the train language which has the minimum score (closest language in the train set)\n",
        "    # index = np.argmin(D.iloc[ii,:])\n",
        "    \n",
        "    # print(index)\n",
        "\n",
        "  return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iObEQwV9R08N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Highlighting the limits of your model\n",
        "\n",
        "# no spatial order\n",
        "# size of the feature space increase as bigger n grams are used\n",
        "# charcter level doesnot capture word level information "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalAAc9PQW7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # prepare the train and test data\n",
        "\n",
        "# # split data into train and test set\n",
        "# test_data_size = .8 # use 20% data for testing, 80% for training\n",
        "\n",
        "# # get all the words for each language and save it in a list\n",
        "# word_list = [i.split() for i in  docs]\n",
        "\n",
        "# # generate training and test data for each language using a map fucntion\n",
        "# # we will split the words in each language into train and test\n",
        "# # this ignores the temporal order of the words\n",
        "# split_data = lambda x:  train_test_split(x,test_size=test_data_size)\n",
        "\n",
        "# # get a list which contains the test and train data splits\n",
        "# z = list(map(split_data, word_list))\n",
        "# # z2 = [train_test_split(x,test_size=.2,radnom_state=42) for x in word_list];\n",
        "\n",
        "# # index the train and test data from the list z\n",
        "# train_data = [item[0] for item in z]\n",
        "# test_data = [item[1] for item in z]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}